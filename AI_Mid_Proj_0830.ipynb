{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00de6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a213b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(input_file) :\n",
    "    data = pd.read_csv(input_file, encoding=\"UTF-8\")\n",
    "    #reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    #_, csv_row = reader.read(input_file)\n",
    "    \n",
    "    data.head()\n",
    "    \n",
    "    features = data.copy()\n",
    "    labels = features.pop('Price')\n",
    "    features = np.array(features)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4a82847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(data):\n",
    "    normalize = preprocessing.Normalization()\n",
    "    normalize.adapt(data)\n",
    "    return normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0d6292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels) :\n",
    "    print(features)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        Normalize(features),\n",
    "        layers.Dense(16),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = tf.losses.MeanSquaredError(), \n",
    "                  optimizer = tf.optimizer.Adam())\n",
    "    model.fit(features, labels, ephochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf4df705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaodData\n",
      "[['Maruti Wagon R LXI CNG' 'Mumbai' 2010 ... 1 11 1]\n",
      " ['Hyundai Creta 1.6 CRDi SX Option' 'Pune' 2015 ... 2 2 2]\n",
      " ['Honda Jazz V' 'Chennai' 2011 ... 1 11 1]\n",
      " ...\n",
      " ['Mahindra Xylo D4 BSIV' 'Jaipur' 2012 ... 1 11 1]\n",
      " ['Maruti Wagon R VXI' 'Kolkata' 2013 ... 2 2 2]\n",
      " ['Chevrolet Beat Diesel' 'Hyderabad' 2011 ... 1 11 1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Maruti Wagon R LXI CNG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ebe329a5f364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/ev-train-data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-38717f0c7896>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(features, labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     model = tf.keras.Sequential([\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-b0cb39ab2d55>\u001b[0m in \u001b[0;36mNormalize\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnormalize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, reset_state)\u001b[0m\n\u001b[0;32m    211\u001b[0m       \u001b[1;31m# until we've gotten an exception indicating that we have no more data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0maccumulator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[0mdata_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;31m# Note that this belongs to the outer indentation of 'try' - we need to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\normalization.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(self, values, accumulator)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[0mreduction_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduction_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduction_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3438\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3440\u001b[1;33m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[0;32m   3441\u001b[0m                           out=out, **kwargs)\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Maruti Wagon R LXI CNG'"
     ]
    }
   ],
   "source": [
    "f, l = LoadData('data/ev-train-data.csv')\n",
    "train(f, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54e91e",
   "metadata": {},
   "source": [
    "## 다중선형회귀 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acbf3b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   23879.3008\n",
      "   50 |     294.6528\n",
      "  100 |      32.8810\n",
      "  150 |      29.9010\n",
      "  200 |      29.7929\n",
      "  250 |      29.7167\n",
      "  300 |      29.6412\n",
      "  350 |      29.5659\n",
      "  400 |      29.4909\n",
      "  450 |      29.4158\n",
      "  500 |      29.3411\n",
      "  550 |      29.2667\n",
      "  600 |      29.1923\n",
      "  650 |      29.1181\n",
      "  700 |      29.0443\n",
      "  750 |      28.9705\n",
      "  800 |      28.8970\n",
      "  850 |      28.8237\n",
      "  900 |      28.7506\n",
      "  950 |      28.6775\n",
      " 1000 |      28.6049\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "F_CNT = 16\n",
    "\n",
    "x1 = [73, 93, 89, 96, 73]\n",
    "x2 = [80, 88, 91, 98, 66]\n",
    "x3 = [75, 93, 90, 100, 70]\n",
    "y = [152, 185, 180, 196, 142]\n",
    "\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1) : \n",
    "    with tf.GradientTape() as tape:\n",
    "        h = w1*x1 + w2*x2 + w3*x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(h-y))\n",
    "        \n",
    "        w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "        \n",
    "        w1.assign_sub(learning_rate * w1_grad)\n",
    "        w2.assign_sub(learning_rate * w2_grad)\n",
    "        w3.assign_sub(learning_rate * w3_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "        if i%50 == 0:\n",
    "            print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cd5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
